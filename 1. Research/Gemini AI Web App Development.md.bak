

# **Architectural Blueprint and Implementation Guide: VNLaw AI Conversational Assistant**

## **Executive Summary**

This report provides a comprehensive architectural blueprint and a detailed implementation guide for developing a secure, enterprise-grade conversational AI web application for VNLaw. The proposed application will replace the existing Google Chat bot interface, transitioning from a simple search function to a sophisticated chat experience powered by Google's Vertex AI generative models. This initiative is assessed as **highly feasible**, leveraging mature and fully managed services within the Google Cloud Platform (GCP).1

The project's complexity is rated as **moderate**. This assessment reflects the need for careful integration and orchestration across several distinct but well-documented domains: serverless application hosting, advanced AI/ML service integration, robust identity and access management, and production-grade network engineering. While the individual components are designed to simplify development, their successful combination requires a structured architectural approach.

The core recommendation is a modern, serverless architecture centered on four key GCP services:

1. **Google Cloud Run:** For hosting the backend web application in a scalable, cost-effective, and portable containerized environment.  
2. **Vertex AI RAG Engine:** To serve as the intelligent backend, seamlessly connecting your existing Vertex AI Search data stores with powerful Gemini chat models to deliver contextually aware, factually grounded responses.  
3. **Google OAuth 2.0 (Internal User Type):** To provide secure, frictionless authentication, strictly limiting access to users with a valid @vnlaw.com.vn Google Workspace account.  
4. **Global Application Load Balancer:** To map the custom domain www.ai.vnlaw.com.vn to the application, manage SSL/TLS certificates, and provide a secure, stable entry point for user traffic.

The successful implementation of this project will deliver significant business value. It represents a strategic evolution from a limited-scope search bot to a powerful conversational AI platform. This new tool will enhance knowledge discovery, improve the efficiency of legal research, and provide all authorized personnel at vnlaw.com.vn with a secure, intuitive interface to interact with the firm's curated knowledge base.

---

## **Part 1: Strategic Analysis and Architectural Blueprint**

This section establishes the strategic foundation for the project, detailing the feasibility and complexity of the initiative and presenting a robust architectural blueprint. It provides a clear rationale for the selection of each core technology, ensuring the proposed solution is not only technically sound but also strategically aligned with long-term goals of scalability, security, and maintainability.

### **1.1 Feasibility and Complexity Assessment**

A thorough analysis of the project requirements against the capabilities of the Google Cloud Platform confirms that the proposed web application is entirely feasible. The foundational AI assets, specifically the "AI Search at VNLaw Precedent 1" application and its associated data stores ("VNLaw Precedent 1" and "VNLaw Infobank 1"), are already established within the "VNLaw-AI-Development" GCP project (Image 1, Image 2). These existing resources serve as a critical data layer that can be directly leveraged by the new system, significantly de-risking the AI integration component. Google Cloud provides a complete suite of managed services for hosting, authentication, and advanced AI, making it the ideal platform for this initiative.1

The project's overall complexity is moderate, with varying levels across its constituent parts:

* **Cloud Infrastructure (Low-to-Medium Complexity):** The adoption of serverless platforms like Cloud Run and managed AI services like the RAG Engine dramatically reduces the burden of infrastructure management.4 The complexity is concentrated in the initial configuration, networking setup (VPC, Load Balancer), and Identity and Access Management (IAM) policies required to securely interconnect these services.  
* **AI Backend Integration (Medium Complexity):** This represents the most significant architectural evolution. The task involves shifting from making a direct API call to Vertex AI Search to interacting with the more sophisticated Vertex AI RAG Engine.8 This is a move from a simple information retrieval pattern to a more complex retrieve-then-generate workflow. Success requires a solid understanding of the Retrieval-Augmented Generation (RAG) process and the correct implementation of the Vertex AI SDK to orchestrate the interaction between the retriever (your existing data store) and the generator (a Gemini model).3  
* **Authentication & Security (Low Complexity):** The strict requirement to limit access to the vnlaw.com.vn domain is addressed elegantly and securely by configuring the application's Google OAuth 2.0 consent screen for "Internal" users.12 This is a standard, well-documented GCP feature that leverages your existing Google Workspace as the identity provider, offloading the complexities of user database management, password policies, and multi-factor authentication to Google's robust infrastructure.  
* **Application Development (Medium Complexity):** The existing codebase, presumably developed for the Google Chat bot framework, will require substantial refactoring. The event-driven, webhook-based logic of a chat bot 14 must be transformed into a standard web application architecture with RESTful API endpoints, user session management, and a server-side OAuth 2.0 flow. This is a standard software engineering task but should not be underestimated.

### **1.2 Proposed Solution Architecture**

The recommended architecture is designed for security, scalability, and maintainability, utilizing a suite of fully managed Google Cloud services. The following diagram and description outline the flow of a user interaction from login to receiving an AI-generated response.

*(Architectural Diagram would be placed here, visually representing the flow described below.)*

1. **User Access:** An authenticated employee with a @vnlaw.com.vn account navigates to https://www.ai.vnlaw.com.vn in their web browser.  
2. **DNS Resolution & Load Balancing:** The domain's DNS A record points to the static IP address of a **Global Application Load Balancer**. The load balancer terminates the HTTPS connection using a Google-managed SSL certificate, ensuring secure communication.  
3. **Request Forwarding:** The load balancer forwards the user's request to the backend service, which is configured as a **Serverless Network Endpoint Group (NEG)** pointing to the **Cloud Run** service.  
4. **Authentication Trigger:** The Cloud Run backend application receives the request. If the user does not have an active session, the application initiates the Google OAuth 2.0 authorization code flow. It redirects the user's browser to Google's sign-in page, including its unique Client ID and the required scopes.  
5. **Domain-Restricted Consent:** Google presents the OAuth consent screen. Because the application is configured as **"Internal,"** only users logged in with a @vnlaw.com.vn account are permitted to proceed. Any other user (e.g., @gmail.com) would be denied access at this stage.12  
6. **Authorization and Callback:** After the user consents, Google redirects the browser back to the application's pre-configured callback URL (e.g., /auth/callback), providing an authorization code. The backend application exchanges this code, along with its Client Secret, for an access token and an ID token. It validates the ID token to confirm the user's identity and establishes a secure session.  
7. **AI Query Processing:** With an authenticated session, the user submits a query through the web interface. The Cloud Run backend receives this query. Acting with the identity of its attached **Service Account**, it makes a secure API call to the **Vertex AI RAG Engine**.  
8. **RAG Engine Orchestration:** The RAG Engine executes the core AI workflow:  
   * **Retrieval:** It queries the configured **RAG Corpus**, which is linked to the existing "AI Search at VNLaw Precedent 1" Vertex AI Search data store. It retrieves the most relevant text chunks from your legal documents based on the user's query.4  
   * **Generation:** The engine automatically combines the retrieved, factual context with the original user query into an augmented prompt. This prompt is then sent to a powerful **Vertex AI Gemini Model** (e.g., Gemini 1.5 Flash). The model generates a coherent, conversational response that is factually grounded in the retrieved information.9  
9. **Response Delivery:** The generated response is sent back to the Cloud Run service, which then streams it to the user's web browser, providing a real-time, interactive chat experience.

### **1.3 Core Technology Rationale**

The selection of each technology in the proposed architecture is deliberate, aimed at maximizing performance, security, and developer productivity while minimizing operational overhead.

#### **Application Hosting: Why Cloud Run is the Optimal Choice**

For a new serverless application on Google Cloud, Cloud Run is the definitive choice over its predecessor, App Engine. This recommendation is rooted in Google's own strategic direction and a clear technical superiority for this use case. Google explicitly recommends Cloud Run for new projects, describing it as the "latest evolution of Google Cloud Serverless" that builds upon over a decade of experience from running App Engine.7

The adoption of Cloud Run's container-based deployment model is more than a technical detail; it represents a beneficial evolution for the development team's workflow. While App Engine's traditional model focused on deploying source code directly, Cloud Run requires the deployment of a standardized container image.15 This necessitates the adoption of

Dockerfile creation and container image management (e.g., via Google Artifact Registry) into the development lifecycle. Although this introduces an initial learning curve, it aligns the team with modern DevOps and CI/CD best practices, enhances environmental consistency from development to production, and ensures the application is fundamentally portable, avoiding vendor lock-in.17 This project can thus serve as a catalyst for modernizing the entire software development and deployment process at VNLaw.

| Feature | Cloud Run | App Engine (Standard Gen 2\) | Justification for VNLaw |
| :---- | :---- | :---- | :---- |
| **Deployment Unit** | Standard Docker Container 15 | Source Code / Container 15 | **Cloud Run.** Provides maximum flexibility and portability. Avoids lock-in to specific runtimes or platform APIs. |
| **Scalability** | Scales up rapidly and down to **zero** 15 | Scales up rapidly and down to **zero** 15 | **Tie.** Both offer excellent scale-to-zero capabilities, which is ideal for cost efficiency. |
| **Cost Model** | Pay-per-use (CPU, memory, requests) 16 | Pay-per-use (instance hours) 16 | **Cloud Run.** The pay-per-use model is highly cost-effective for applications with variable traffic, as there are no charges for idle time. |
| **Language Support** | Any language/framework 18 | Limited set of runtimes (Python, Go, etc.) 15 | **Cloud Run.** Unrestricted choice of technology stack provides future-proofing and allows use of the best tool for the job. |
| **Request Timeout** | Up to 60 minutes 1 | Up to 60 minutes | **Tie.** Both platforms now support long-running requests, suitable for complex AI queries. |
| **Strategic Direction** | Recommended for new projects 7 | Legacy, with new features prioritizing Cloud Run 17 | **Cloud Run.** Aligns with Google's long-term serverless strategy, ensuring access to future innovations and support. |

#### **AI Engine: The Power of Vertex AI RAG Engine**

The core challenge with using large language models (LLMs) like Gemini in an enterprise context is their lack of knowledge about private, domain-specific data. A standard Gemini model does not know the contents of the "VNLaw Precedent 1" data store and would generate generic or incorrect ("hallucinated") answers to specific legal queries.19 The solution is Retrieval-Augmented Generation (RAG), a technique that grounds the LLM's response in factual information retrieved from a trusted knowledge base.4

The **Vertex AI RAG Engine** is a managed service that orchestrates this entire complex pipeline, from data retrieval to final response generation.10 This is a critical advantage, as it abstracts away the significant engineering effort required to build, optimize, and maintain a custom RAG system. Your development team can focus on the application's user experience and business logic instead of the underlying AI infrastructure.4

A key benefit of this approach is its ability to directly integrate with your existing assets. The RAG Engine is designed to use Vertex AI Search data stores as a retrieval backend.8 This means the investment already made in creating and populating the "AI Search at VNLaw Precedent 1" data store can be leveraged immediately, without a costly and time-consuming data re-ingestion or re-processing phase.20 This seamless integration dramatically accelerates the development timeline.

Furthermore, choosing a managed orchestration layer like the RAG Engine provides significant long-term architectural agility. It effectively decouples the three core components of the system: the frontend application, the data retriever (Vertex AI Search), and the language generator (Gemini). This separation means that in the future, you can:

* Upgrade to a newer, more powerful Gemini model by simply changing a model identifier in the application code, without affecting the data or retrieval logic.  
* Expand the AI's knowledge base by adding new data stores (e.g., documents from Google Drive, data from other corporate systems) to the RAG Corpus, often without requiring any changes to the application code itself.20  
* Experiment with different data chunking or indexing strategies within your Vertex AI Search data stores to improve retrieval relevance, again, without modifying the core application.

This architectural choice prevents the system from becoming a rigid monolith and ensures it can adapt and evolve as both AI technology and the firm's data landscape change over time.

#### **Authentication: Google OAuth 2.0 with "Internal" User Type**

The requirement for strict, domain-level access control is best met by leveraging your existing Google Workspace identity provider through Google OAuth 2.0. This approach offers superior security and a seamless user experience.22 By using Google as the identity provider, you eliminate the need to build and maintain a separate user management system, including functionalities like password storage, reset flows, and multi-factor authentication.

The critical configuration is setting the application's "User Type" to **Internal** on the OAuth consent screen.12 This is the most direct and secure method to enforce the

@vnlaw.com.vn domain restriction. It programmatically prevents any user account outside of your Google Workspace organization from authenticating or granting consent to the application. For employees, this provides a frictionless single sign-on (SSO) experience, allowing them to log in with their familiar corporate Google account.

#### **Networking: Global Application Load Balancer for Custom Domains**

While Cloud Run offers a native domain mapping feature, it remains in a "Preview" state and is explicitly not recommended for production services due to potential latency and limited support.24 The robust, generally available, and fully supported method for mapping a custom domain to a Cloud Run service is to use a

**Global Application Load Balancer**.24

This approach provides several production-grade advantages:

* **Enhanced Security:** The load balancer provides a stable, static IP address and can be integrated with Google Cloud Armor, a web application firewall (WAF), to protect against DDoS attacks and other common web threats. It also allows for the enforcement of modern, strict TLS policies, which is not possible with the native mapping feature.24  
* **Centralized SSL/TLS Management:** The load balancer automatically provisions and renews Google-managed SSL certificates for your custom domain, simplifying certificate management.  
* **Improved Architecture:** Using a load balancer creates a clean separation of concerns. The Cloud Run service can be configured with stricter ingress controls, allowing traffic only from the load balancer. This reduces the application's direct exposure to the public internet and minimizes its attack surface, forming a key part of a defense-in-depth security strategy.

---

## **Part 2: Step-by-Step Implementation Guide**

This section provides a granular, phased guide for the engineering team to implement the proposed architecture. Each phase details the necessary steps, configurations, and commands required to build and deploy the VNLaw AI Conversational Assistant.

### **Phase 1: Foundational GCP Setup**

This phase establishes the core IAM identities and enables the necessary APIs for the project.

#### **1.1 Create a Dedicated Service Account**

A dedicated service account will act as the non-human identity for the Cloud Run service, allowing it to securely interact with other Google Cloud APIs without using personal user credentials or long-lived keys.

1. Navigate to **IAM & Admin \> Service Accounts** in the Google Cloud Console.  
2. Click **Create Service Account**.  
3. Enter a **Service account name**, such as sa-vnlaw-ai-webapp.  
4. The **Service account ID** will be automatically populated.  
5. Provide a clear **Description**, for example, "Service account for the VNLaw AI Web Application on Cloud Run."  
6. Click **Create and Continue**. Skip granting access to users and click **Done**. This identity will be attached directly to the Cloud Run service during deployment.26

#### **1.2 Grant Necessary IAM Permissions**

Assign the required roles to the newly created service account to grant it the permissions needed to function.

1. Navigate to **IAM & Admin \> IAM**.  
2. Click **Grant Access**.  
3. In the **New principals** field, enter the email address of the service account created above (e.g., sa-vnlaw-ai-webapp@vnlaw-ai-development.iam.gserviceaccount.com).  
4. In the **Assign roles** dropdown, select the **Vertex AI User** (roles/aiplatform.user) role. This comprehensive role grants the necessary permissions to query the RAG Engine and interact with Gemini models.28  
5. Click **Save**.

The following table summarizes the key IAM roles required for the project.

| Principal | Role | Justification |
| :---- | :---- | :---- |
| sa-vnlaw-ai-webapp@... | Vertex AI User (roles/aiplatform.user) | Allows the Cloud Run service to make authenticated calls to the Vertex AI RAG Engine and Gemini APIs. |
| \`\` | Cloud Run Invoker (roles/run.invoker) | Allows the external load balancer to securely invoke the private Cloud Run service. This is configured during the load balancer setup. |
| @vnlaw.com.vn users | N/A (Authenticated via OAuth) | Users authenticate via the application's OAuth flow, not direct IAM roles, to access the application's functionality. |

For enhanced security in a mature production environment, the broad Vertex AI User role can be replaced with a custom IAM role containing only the specific permissions required, such as aiplatform.endpoints.predict and aiplatform.ragCorpora.query.

#### **1.3 Enable Required APIs**

Ensure all necessary Google Cloud services are enabled for the project.

1. Navigate to **APIs & Services \> Library**.  
2. Search for and enable each of the following APIs:  
   * **Vertex AI API** (aiplatform.googleapis.com) 14  
   * **Cloud Run Admin API** (run.googleapis.com) 14  
   * **Compute Engine API** (compute.googleapis.com) (Required for Load Balancer configuration)  
   * **Cloud Build API** (cloudbuild.googleapis.com) (For automating container builds)  
   * **Artifact Registry API** (artifactregistry.googleapis.com) (For storing container images)  
   * **Secret Manager API** (secretmanager.googleapis.com) (For securely storing credentials)

### **Phase 2: Authentication Configuration**

This phase configures the Google OAuth 2.0 flow to handle user authentication securely.

#### **2.1 Configure the OAuth Consent Screen**

1. Navigate to **APIs & Services \> OAuth consent screen**.  
2. Under **User Type**, select **Internal**. This is the most critical step for restricting access to your organization.12  
3. Click **Create**.  
4. Fill in the required application details:  
   * **App name:** VNLaw AI Assistant  
   * **User support email:** Select an appropriate support email address.  
   * **App logo:** (Optional) Upload a logo.  
   * **Authorized domains:** Ensure vnlaw.com.vn is listed.  
   * **Developer contact information:** Enter an email for Google to send project notifications.  
5. Click **Save and Continue** through the "Scopes" and "Optional info" steps. An internal app does not require scope verification.12  
6. Review the summary and click **Back to Dashboard**.

#### **2.2 Create OAuth 2.0 Client ID**

1. Navigate to **APIs & Services \> Credentials**.  
2. Click **\+ Create Credentials** and select **OAuth client ID**.  
3. For **Application type**, select **Web application**.22  
4. Enter a **Name**, such as VNLaw AI Web App Client.  
5. Under **Authorized JavaScript origins**, click **\+ Add URI** and enter https://www.ai.vnlaw.com.vn.30  
6. Under **Authorized redirect URIs**, click **\+ Add URI** and enter https://www.ai.vnlaw.com.vn/auth/callback. This endpoint must be implemented in your backend application to handle the final step of the OAuth flow.23  
7. Click **Create**.  
8. A dialog will appear with your **Client ID** and **Client Secret**. Copy both values immediately and store them securely. It is highly recommended to store these in **Google Cloud Secret Manager** rather than committing them to source code.

### **Phase 3: AI Backend Implementation (RAG)**

This phase focuses on connecting the application to the Vertex AI RAG Engine.

#### **3.1 Configure Vertex AI RAG Engine with Existing Data Store**

The application will programmatically create a RAG Corpus that points to your existing Vertex AI Search data store. This is done using the Vertex AI SDK for Python.

1. **Install the SDK:** pip install \--upgrade google-cloud-aiplatform  
2. **Identify Data Store Details:** From the GCP Console (Image 2), find the **Project ID**, **Location** (e.g., global), and the **ID** of your "VNLaw Precedent 1" data store.  
3. **Create the RAG Corpus:** Use the following Python code snippet as a template. This code should be part of an initialization script or the application's startup logic.

Python

from google.cloud import aiplatform

\# \--- Configuration \---  
PROJECT\_ID \= "vnlaw-ai-development"  
LOCATION \= "us-central1"  \# Or your preferred RAG Engine supported region  
CORPUS\_DISPLAY\_NAME \= "VNLaw\_Legal\_Precedents\_Corpus"  
DATA\_STORE\_PROJECT\_ID \= "vnlaw-ai-development"  
DATA\_STORE\_LOCATION \= "global"  \# Location of your Vertex AI Search data store  
DATA\_STORE\_ID \= "vnlaw-precedent-1\_1755248534491" \# Example ID from Image 2

def create\_rag\_corpus\_with\_search(  
    project\_id: str,  
    location: str,  
    display\_name: str,  
    vs\_project\_id: str,  
    vs\_location: str,  
    vs\_datastore\_id: str,  
) \-\> str:  
    """Creates a RAG Corpus linked to an existing Vertex AI Search data store."""  
    aiplatform.init(project=project\_id, location=location)  
      
    \# Define the Vertex AI Search data store configuration  
    vertex\_ai\_search\_store\_config \= {  
        "project\_id": vs\_project\_id,  
        "location": vs\_location,  
        "data\_store\_id": vs\_datastore\_id,  
    }

    \# Create the RAG Corpus  
    rag\_corpus \= aiplatform.rag.create\_corpus(  
        display\_name=display\_name,  
        rag\_resources=\[  
            {  
                "rag\_file\_ids":, \# Empty as we are using a data store  
                "vertex\_ai\_search\_store": vertex\_ai\_search\_store\_config,  
            }  
        \],  
    )  
      
    print(f"Created RAG Corpus: {rag\_corpus.name}")  
    return rag\_corpus.name

\# \--- Execute creation \---  
\# rag\_corpus\_resource\_name \= create\_rag\_corpus\_with\_search(...)

This script explicitly uses the vertex\_ai\_search\_store configuration to link the new RAG Corpus directly to your existing data, as documented in the integration guide.8

#### **3.2 Develop the Backend API**

Use a modern Python web framework like FastAPI to build the backend service.

1. **Project Structure:** Set up a project with a main application file (e.g., main.py), a requirements.txt file, and modules for different concerns (e.g., auth.py, chat.py).  
2. **API Endpoint:** Create a streaming API endpoint to handle chat requests.

Python

\# Example using FastAPI in main.py  
from fastapi import FastAPI, Request  
from fastapi.responses import StreamingResponse  
from google.cloud import aiplatform  
import vertexai  
from vertexai.generative\_models import GenerativeModel, Tool

\# \--- Initialization \---  
app \= FastAPI()  
vertexai.init(project=PROJECT\_ID, location=LOCATION)

RAG\_CORPUS\_RESOURCE\_NAME \= "projects/.../locations/.../ragCorpora/..." \# Get this from step 3.1

\# Create a RAG retrieval tool  
rag\_retrieval\_tool \= Tool.from\_retrieval(  
    retrieval=aiplatform.rag.Retrieval(  
        source=aiplatform.rag.VertexAIRagStore(  
            rag\_corpora=  
        )  
    )  
)

\# Instantiate the Gemini model  
model \= GenerativeModel("gemini-1.5-flash-001")

@app.post("/api/chat")  
async def handle\_chat(request: Request):  
    \# NOTE: Add robust authentication check here to verify user session  
    \# This is a placeholder for the actual auth logic  
      
    data \= await request.json()  
    user\_query \= data.get("query")

    async def event\_stream():  
        \# Generate content using the model and the RAG tool  
        response\_stream \= model.generate\_content(  
            user\_query,  
            tools=\[rag\_retrieval\_tool\],  
            stream=True  
        )  
        for chunk in response\_stream:  
            yield f"data: {chunk.text}\\n\\n"

    return StreamingResponse(event\_stream(), media\_type="text/event-stream")

\# Add endpoints for /login and /auth/callback to handle the OAuth 2.0 flow

This code demonstrates how to initialize the Vertex AI client, define the RAG Corpus as a Tool for the Gemini model, and create a streaming endpoint. The call to generate\_content with the tools parameter instructs Gemini to use the RAG Engine to retrieve context before generating an answer.8

### **Phase 4: Application Containerization and Deployment**

This phase packages the application into a container and deploys it to Cloud Run.

#### **4.1 Create a Dockerfile**

Create a file named Dockerfile in your project's root directory.

Dockerfile

\# Use an official Python runtime as a parent image  
FROM python:3.11\-slim

\# Set the working directory in the container  
WORKDIR /app

\# Copy the requirements file into the container  
COPY requirements.txt.

\# Install any needed packages specified in requirements.txt  
RUN pip install \--no-cache-dir \-r requirements.txt

\# Copy the rest of the application's code into the container  
COPY..

\# Expose the port the app runs on  
EXPOSE 8080

\# Define the command to run the application  
\# Use Gunicorn for a production-ready web server  
CMD

#### **4.2 Set up Artifact Registry**

1. Navigate to **Artifact Registry**.  
2. Click **Create Repository**.  
3. Enter a **Name** (e.g., vnlaw-ai-apps).  
4. Select **Docker** as the format.  
5. Choose a region for the repository.  
6. Click **Create**.

#### **4.3 Build and Push the Container Image**

Use Google Cloud Build for automated, serverless builds. Create a cloudbuild.yaml file.

YAML

steps:  
\- name: 'gcr.io/cloud-builders/docker'  
  args:  
\- name: 'gcr.io/cloud-builders/docker'  
  args:  
images:  
\- '${\_LOCATION}-docker.pkg.dev/${PROJECT\_ID}/${\_REPO\_NAME}/${\_IMAGE\_NAME}:$COMMIT\_SHA'

Submit the build using the gcloud CLI: gcloud builds submit \--config cloudbuild.yaml \--substitutions=\_LOCATION=,\_REPO\_NAME=vnlaw-ai-apps,\_IMAGE\_NAME=vnlaw-chat-backend

#### **4.4 Deploy to Cloud Run**

Deploy the container image to a new Cloud Run service.

Bash

gcloud run deploy vnlaw-chat-backend \\  
  \--image-docker.pkg.dev///\[IMAGE\_NAME\]: \\  
  \--platform managed \\  
  \--region \\  
  \--service-account "sa-vnlaw-ai-webapp@.iam.gserviceaccount.com" \\  
  \--ingress internal-and-cloud-load-balancing \\  
  \--allow-unauthenticated \\  
  \--set-secrets "OAUTH\_CLIENT\_ID=vnlaw-oauth-client-id:latest" \\  
  \--set-secrets "OAUTH\_CLIENT\_SECRET=vnlaw-oauth-client-secret:latest"

* \--service-account: Attaches the dedicated identity created in Phase 1\.26  
* \--ingress internal-and-cloud-load-balancing: A critical security measure that restricts direct internet access, allowing invocations only from the load balancer.  
* \--allow-unauthenticated: This may seem counterintuitive, but it's necessary because authentication will be handled by *your application's OAuth flow*, not IAM. The load balancer itself is a trusted, unauthenticated source from Cloud Run's perspective.  
* \--set-secrets: Securely mounts the OAuth credentials from Secret Manager as environment variables.

### **Phase 5: Networking and Custom Domain Mapping**

This final phase exposes the application to users via the custom domain.

#### **5.1 Create a Serverless Network Endpoint Group (NEG)**

1. Navigate to **Compute Engine \> Network endpoint groups**.  
2. Click **Create network endpoint group**.  
3. Enter a **Name** (e.g., neg-vnlaw-chat-backend).  
4. Select **Serverless network endpoint group** as the type.  
5. Choose the **Region**.  
6. Under **Cloud Run**, select the vnlaw-chat-backend service.  
7. Click **Create**.25

#### **5.2 Configure the Global Application Load Balancer**

1. Navigate to **Network Services \> Load balancing**.  
2. Click **Create load balancer**.  
3. Under **Application Load Balancer (HTTP/S)**, click **Start configuration**.  
4. Select **From Internet to my VMs or serverless services** and **Global external Application Load Balancer**.  
5. **Backend configuration:**  
   * Click **Create a backend service**.  
   * Enter a **Name** (e.g., bs-vnlaw-chat-backend).  
   * For **Backend type**, select **Serverless NEG**.  
   * In the **Backends** section, select the NEG created in the previous step.  
   * Click **Create**.  
6. **Frontend configuration:**  
   * Enter a **Name** (e.g., fe-vnlaw-chat-frontend).  
   * Set **Protocol** to **HTTPS (HTTP/2)**.  
   * Under **IP address**, click **Create IP address** to reserve a new static IP.  
   * Under **Certificate**, click **Create a new certificate**. Select **Google-managed certificate** and enter www.ai.vnlaw.com.vn as the domain.  
7. **Routing rules:** Ensure the default rule routes traffic from the frontend to the backend service.  
8. Review and click **Create**.

#### **5.3 Update DNS Records**

The final step is to point your custom domain to the load balancer's static IP address.

1. After the load balancer is created, find and copy its static IP address from the frontend configuration.  
2. Log in to your domain registrar's control panel for vnlaw.com.vn.  
3. Navigate to the DNS management or host records section.  
4. Create a new DNS record with the following details 24:

| Record Type | Host/Name | Value / Points To | TTL (Seconds) |
| :---- | :---- | :---- | :---- |
| A | www.ai | \`\` | 3600 |

After the DNS changes propagate (which can take from a few minutes to several hours), users will be able to access the application at https://www.ai.vnlaw.com.vn.

---

## **Part 3: Codebase Adaptation and Best Practices**

This section addresses the necessary refactoring of the existing codebase and provides guidance for developing the web frontend.

### **3.1 Analysis and Refactoring of main.py and oauth\_service.py**

The existing codebase, designed for a Google Chat bot, operates on a fundamentally different paradigm than a web application. A Google Chat bot is typically a webhook that responds to events pushed from the Chat service, such as a user sending a message or using a slash command.14 The

main.py file likely contains a function (often using Flask) that parses these incoming JSON event payloads and formulates a response in the format expected by the Chat API. The oauth\_service.py file might handle verification tokens or service-to-service authentication specific to the Google Chat platform.

This logic is not directly portable. A complete refactoring is required:

1. **Extract Business Logic:** The core logic for interacting with the Vertex AI service should be isolated from the Chat-specific event handling code. This function, which takes a user query and returns an AI response, will be the centerpiece of the new application.  
2. **Replace Webhook with RESTful API:** The webhook entry point must be replaced with standard RESTful API endpoints using a web framework like FastAPI or Flask. This includes:  
   * An endpoint to initiate the OAuth 2.0 login flow (e.g., GET /login).  
   * The callback endpoint specified in the OAuth client configuration to handle the response from Google (e.g., GET /auth/callback).  
   * The primary chat endpoint that will receive user queries from the authenticated frontend (e.g., POST /api/chat).  
3. **Implement Server-Side OAuth 2.0 Flow:** The authentication logic must be rewritten to implement the standard OAuth 2.0 Authorization Code Flow for web server applications.31 This involves managing user sessions (e.g., via secure, HTTP-only cookies) to maintain the authenticated state between the user's browser and the backend service.

### **3.2 Frontend Considerations**

While the detailed implementation of the web frontend is outside the scope of this document, the backend architecture has specific implications for the frontend development team.

* **API Contract:** The backend will expose a primary endpoint, POST /api/chat, which expects a JSON payload containing the user's query (e.g., {"query": "..."}) and will return a streaming response.  
* **Authentication Flow:** The frontend should not handle OAuth tokens directly. Instead, when a user needs to log in, it should simply redirect the browser to the backend's /login endpoint. The backend will manage the entire interaction with Google and establish a session cookie upon successful authentication. The frontend's responsibility is to check for the presence of this session cookie to determine the user's login status.  
* **Handling Streaming Responses:** To create a dynamic, real-time chat experience where the AI's response appears token by token, the frontend must be capable of consuming a streaming HTTP response. This is typically achieved using the Fetch API with a ReadableStream in JavaScript or by using the Server-Sent Events (SSE) protocol. This is a critical user experience requirement.

The transition to a RAG-based system elevates the importance of the underlying data's quality and structure. In a simple search system, a user can easily discard an irrelevant result. In a RAG system, irrelevant or poorly structured retrieved content will directly lead to a factually incorrect or nonsensical generated answer.9 This creates a high-stakes "garbage in, garbage out" scenario. Therefore, a new, ongoing operational process for data governance and optimization of the "VNLaw Precedent 1" data store becomes essential. The team must consider optimal document chunking strategies, the use of metadata for better filtering, and a clear process for keeping source documents current to ensure the continued reliability and accuracy of the AI assistant's output.

---

## **Conclusion and Future Enhancements**

The proposed architecture provides a robust, secure, and scalable foundation for the VNLaw AI Conversational Assistant. By leveraging Google Cloud's managed serverless, AI, and networking services, VNLaw can deploy a powerful, enterprise-grade application while minimizing operational overhead and focusing on core business value. The project is highly feasible, and the step-by-step guide provides a clear and actionable path for implementation.

**Immediate Next Steps:**

1. Provision the dedicated service account and configure all necessary IAM permissions and APIs as outlined in Phase 1\.  
2. Configure the OAuth consent screen and create the OAuth 2.0 Client ID as detailed in Phase 2\. Securely store the credentials in Secret Manager.  
3. Begin development of the backend application, starting with the RAG Engine integration and the server-side OAuth 2.0 flow.  
4. Concurrently, begin development of the web frontend, ensuring it can handle the authentication redirects and streaming API responses.  
5. Set up the Cloud Build and Artifact Registry pipeline to prepare for automated deployments.

Roadmap for Future Enhancements:  
Upon successful launch, the platform can be further enhanced:

* **Formalize CI/CD:** Integrate the Cloud Build process with a source code repository (e.g., GitHub, Cloud Source Repositories) to create a full continuous integration and continuous deployment (CI/CD) pipeline that automatically builds and deploys the application upon code commits.  
* **Comprehensive Monitoring:** Implement structured logging within the Cloud Run application. Create custom dashboards in Cloud Monitoring to track key metrics such as API latency, error rates, user engagement, and Vertex AI token consumption to gain operational insights and manage costs.  
* **User Feedback Loop:** Incorporate a simple feedback mechanism (e.g., thumbs up/down) on each AI-generated response. Log this feedback to identify areas where the RAG system's retrieval or the LLM's generation can be improved through prompt engineering or data store optimization.  
* **Security Hardening:** As a best practice, transition from the broad, predefined Vertex AI User IAM role to a custom IAM role that grants only the minimal set of permissions required by the application (e.g., aiplatform.ragCorpora.query, aiplatform.endpoints.predict). This adheres to the principle of least privilege and further strengthens the application's security posture.

#### **Works cited**

1. Application Hosting Options \- Google Cloud, accessed September 8, 2025, [https://cloud.google.com/hosting-options](https://cloud.google.com/hosting-options)  
2. en.wikipedia.org, accessed September 8, 2025, [https://en.wikipedia.org/wiki/Google\_Cloud\_Platform](https://en.wikipedia.org/wiki/Google_Cloud_Platform)  
3. ChatVertexAI | ü¶úÔ∏è LangChain, accessed September 8, 2025, [https://python.langchain.com/docs/integrations/chat/google\_vertex\_ai\_palm/](https://python.langchain.com/docs/integrations/chat/google_vertex_ai_palm/)  
4. Vertex AI RAG Engine: A developers tool, accessed September 8, 2025, [https://developers.googleblog.com/en/vertex-ai-rag-engine-a-developers-tool/](https://developers.googleblog.com/en/vertex-ai-rag-engine-a-developers-tool/)  
5. Google Cloud web hosting, accessed September 8, 2025, [https://cloud.google.com/solutions/web-hosting](https://cloud.google.com/solutions/web-hosting)  
6. Create a Generative Chat App with Vertex AI Conversation \- Codelabs, accessed September 8, 2025, [https://codelabs.developers.google.com/codelabs/vertex-ai-conversation](https://codelabs.developers.google.com/codelabs/vertex-ai-conversation)  
7. Compare App Engine and Cloud Run | App Engine migration center ..., accessed September 8, 2025, [https://cloud.google.com/appengine/migration-center/run/compare-gae-with-run](https://cloud.google.com/appengine/migration-center/run/compare-gae-with-run)  
8. Use Vertex AI Search as a retrieval backend using ... \- Google Cloud, accessed September 8, 2025, [https://cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/use-vertexai-search](https://cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/use-vertexai-search)  
9. What is Retrieval-Augmented Generation (RAG)? \- Google Cloud, accessed September 8, 2025, [https://cloud.google.com/use-cases/retrieval-augmented-generation](https://cloud.google.com/use-cases/retrieval-augmented-generation)  
10. Vertex AI RAG Engine overview \- Google Cloud, accessed September 8, 2025, [https://cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/rag-overview](https://cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/rag-overview)  
11. Gemini API in Vertex AI quickstart \- Google Cloud, accessed September 8, 2025, [https://cloud.google.com/vertex-ai/generative-ai/docs/start/quickstart](https://cloud.google.com/vertex-ai/generative-ai/docs/start/quickstart)  
12. Configure the OAuth consent screen and choose scopes | Google ..., accessed September 8, 2025, [https://developers.google.com/workspace/guides/configure-oauth-consent](https://developers.google.com/workspace/guides/configure-oauth-consent)  
13. Setting up OAuth 2.0 \- API Console Help \- Google Help, accessed September 8, 2025, [https://support.google.com/googleapi/answer/6158849?hl=en](https://support.google.com/googleapi/answer/6158849?hl=en)  
14. Manage projects with Google Chat, Vertex AI, and Firestore, accessed September 8, 2025, [https://developers.google.com/workspace/chat/tutorial-project-management](https://developers.google.com/workspace/chat/tutorial-project-management)  
15. What is the difference between Google App Engine and Google Cloud Run?, accessed September 8, 2025, [https://stackoverflow.com/questions/55619286/what-is-the-difference-between-google-app-engine-and-google-cloud-run](https://stackoverflow.com/questions/55619286/what-is-the-difference-between-google-app-engine-and-google-cloud-run)  
16. Building a Node app should I use App Engine or Google Cloud Run? \- Stack Overflow, accessed September 8, 2025, [https://stackoverflow.com/questions/72569884/building-a-node-app-should-i-use-app-engine-or-google-cloud-run](https://stackoverflow.com/questions/72569884/building-a-node-app-should-i-use-app-engine-or-google-cloud-run)  
17. App engine standard vs cloud run use cases : r/googlecloud \- Reddit, accessed September 8, 2025, [https://www.reddit.com/r/googlecloud/comments/napc18/app\_engine\_standard\_vs\_cloud\_run\_use\_cases/](https://www.reddit.com/r/googlecloud/comments/napc18/app_engine_standard_vs_cloud_run_use_cases/)  
18. Cloud Run vs App Engine vs Cloud Function \- Sphere Partners, accessed September 8, 2025, [https://www.sphereinc.com/blogs/when-to-choose-app-engine-vs-cloud-functions-or-cloud-run-in-gcp/](https://www.sphereinc.com/blogs/when-to-choose-app-engine-vs-cloud-functions-or-cloud-run-in-gcp/)  
19. RAG on GCP: A Beginner's Guide to Retrieval-Augmented Generation using Vertex AI and Colab Enterprise | by Fahrudeen Mohamed | Medium, accessed September 8, 2025, [https://medium.com/@fahrudeen/rag-on-gcp-a-beginners-guide-to-retrieval-augmented-generation-using-vertex-ai-and-colab-7bc191768c01](https://medium.com/@fahrudeen/rag-on-gcp-a-beginners-guide-to-retrieval-augmented-generation-using-vertex-ai-and-colab-7bc191768c01)  
20. Building RAG with Vertex AI RAG Engine | by Adityo Pratomo \- Medium, accessed September 8, 2025, [https://adityop.medium.com/building-rag-with-vertex-ai-rag-engine-e04bf9ebfa08](https://adityop.medium.com/building-rag-with-vertex-ai-rag-engine-e04bf9ebfa08)  
21. Google Vertex AI Search \- Python LangChain, accessed September 8, 2025, [https://python.langchain.com/docs/integrations/retrievers/google\_vertex\_ai\_search/](https://python.langchain.com/docs/integrations/retrievers/google_vertex_ai_search/)  
22. Using OAuth 2.0 to Access Google APIs | Authorization, accessed September 8, 2025, [https://developers.google.com/identity/protocols/oauth2](https://developers.google.com/identity/protocols/oauth2)  
23. How to Integrate Google OAuth 2.0 for Third-Party Website Login \- Apidog, accessed September 8, 2025, [https://apidog.com/blog/integrate-google-oauth-2-for-third-party-website-login/](https://apidog.com/blog/integrate-google-oauth-2-for-third-party-website-login/)  
24. Mapping custom domains | Cloud Run Documentation | Google Cloud, accessed September 8, 2025, [https://cloud.google.com/run/docs/mapping-custom-domains](https://cloud.google.com/run/docs/mapping-custom-domains)  
25. Custom Domain Mapping using Google Cloud Load Balancer | by Jay Zadafiya | Medium, accessed September 8, 2025, [https://medium.com/@zadafiyajay2/custom-domain-mapping-using-google-cloud-load-balancer-ab29e2cb4b4a](https://medium.com/@zadafiyajay2/custom-domain-mapping-using-google-cloud-load-balancer-ab29e2cb4b4a)  
26. Use a custom service account | Vertex AI \- Google Cloud, accessed September 8, 2025, [https://cloud.google.com/vertex-ai/docs/general/custom-service-account](https://cloud.google.com/vertex-ai/docs/general/custom-service-account)  
27. Google Vertex AI \- AG2, accessed September 8, 2025, [https://docs.ag2.ai/latest/docs/user-guide/models/google-vertexai/](https://docs.ag2.ai/latest/docs/user-guide/models/google-vertexai/)  
28. Vertex AI access control with IAM | Google Cloud, accessed September 8, 2025, [https://cloud.google.com/vertex-ai/docs/general/access-control](https://cloud.google.com/vertex-ai/docs/general/access-control)  
29. Authenticate to Google Workspace APIs with OAuth 2.0 client credentials | SAP, accessed September 8, 2025, [https://cloud.google.com/sap/docs/abap-sdk/on-premises-or-any-cloud/latest/authentication-oauth-client-credentials](https://cloud.google.com/sap/docs/abap-sdk/on-premises-or-any-cloud/latest/authentication-oauth-client-credentials)  
30. Manage OAuth Clients \- Google Cloud Platform Console Help, accessed September 8, 2025, [https://support.google.com/cloud/answer/15549257?hl=en](https://support.google.com/cloud/answer/15549257?hl=en)  
31. Using OAuth 2.0 for Web Server Applications | Authorization \- Google for Developers, accessed September 8, 2025, [https://developers.google.com/identity/protocols/oauth2/web-server](https://developers.google.com/identity/protocols/oauth2/web-server)  
32. Mapping a Cloud Run Service to a Custom Domain | by Herv√© Kakiang Kyanguinbo, accessed September 8, 2025, [https://medium.com/@kakiang/mapping-a-cloud-run-service-to-a-custom-domain-9c9895037551](https://medium.com/@kakiang/mapping-a-cloud-run-service-to-a-custom-domain-9c9895037551)  
33. How to Map a Custom Domain to Google Cloud Run Service \- YouTube, accessed September 8, 2025, [https://www.youtube.com/watch?v=lDtvpUYAFzA](https://www.youtube.com/watch?v=lDtvpUYAFzA)  
34. Unlocking the Power of RAG: Transforming Knowledge Retrieval with Vertex AI \- Medium, accessed September 8, 2025, [https://medium.com/flat-pack-tech/unlocking-the-power-of-rag-transforming-knowledge-retrieval-with-ai-2eb2c91f9633](https://medium.com/flat-pack-tech/unlocking-the-power-of-rag-transforming-knowledge-retrieval-with-ai-2eb2c91f9633)